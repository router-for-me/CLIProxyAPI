# CLIProxyAPI-Extended

> Fork of CLIProxyAPI with unified Canonical IR translation architecture and new providers (Kiro, Cline, Ollama)

## ðŸ“‹ Contributing Notice

**This is an experimental branch.** I'm sharing this work for the community to use and build upon.

- **Cherry-pick what you need** â€” feel free to take individual features or fixes that are useful for your projects
- **Limited maintenance** â€” I have limited time to review extensive change requests
- **Tested but experimental** â€” the code works in my testing environment, but your mileage may vary
- **Clear solutions only** â€” if you report an issue, please provide a specific fix or clear reproduction steps; I don't have time to investigate vague problem descriptions

Contributions are welcome! Simple bug fixes with ready-to-merge code will likely be accepted. For larger changes or feature requests, consider forking â€” this gives you full control over the direction of your modifications.

## What's Added in This Branch

| Component | Description |
|-----------|-------------|
| **Canonical IR Translator** | Hub-and-spoke architecture for format translation |
| **Ollama API** | Full implementation of Ollama-compatible server |
| **Kiro (Amazon Q)** | New provider with access to Claude via Amazon Q |
| **Cline** | Provider with free models (MiniMax M2, Grok) |
| **Model Registry** | Support for provider:modelID keys, visual prefixes |
| **ThinkingSupport** | Metadata for reasoning-capable models |

---

> **62% codebase reduction** â€” from 13,930 to 5,302 lines  
> **86% Google providers unification** â€” from 5,651 to 780 lines  
> **New providers:** Ollama, Kiro (Amazon Q), Cline (free models)

## Problem

Legacy translator used **NÃ—M architecture** â€” each sourceâ†’target pair required a separate directory with files:

```
internal/translator/
â”œâ”€â”€ openai/          â†’ claude/, gemini/, gemini-cli/, openai/
â”œâ”€â”€ claude/          â†’ gemini/, gemini-cli/, openai/
â”œâ”€â”€ codex/           â†’ claude/, gemini/, gemini-cli/, openai/
â”œâ”€â”€ gemini/          â†’ claude/, gemini/, gemini-cli/, openai/
â”œâ”€â”€ gemini-cli/      â†’ claude/, gemini/, openai/
â””â”€â”€ antigravity/     â†’ claude/, gemini/, openai/
```

**6 sources Ã— 4-5 targets = 27 translation paths, 84 files, massive code duplication.**

## Solution

**Hub-and-spoke architecture** with unified Intermediate Representation (IR):

```
    OpenAI â”€â”€â”€â”€â”€â”                       â”Œâ”€â”€â”€â”€â”€ OpenAI
    Claude â”€â”€â”€â”€â”€â”¤                       â”œâ”€â”€â”€â”€â”€ Claude
    Ollama â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â–º Canonical â—„â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€ Gemini (AI Studio)
      Kiro â”€â”€â”€â”€â”€â”¤       IR              â”œâ”€â”€â”€â”€â”€ Gemini CLI
     Cline â”€â”€â”€â”€â”€â”˜                       â”œâ”€â”€â”€â”€â”€ Antigravity
                                        â”œâ”€â”€â”€â”€â”€ Ollama
                                        â””â”€â”€â”€â”€â”€ Cline
```

**Result:** 15 files (5 parsers + 5 emitters + 5 IR core), minimal duplication.

## Metrics

| Metric                    | Legacy        | Canonical IR  | Î”         |
|---------------------------|---------------|---------------|-----------|
| Files                     | 84            | 15            | **âˆ’82%**  |
| Lines of code             | 13,930        | 5,302         | **âˆ’62%**  |
| Translation paths         | 27            | 10            | **âˆ’63%**  |
| Google providers (lines)  | 5,651         | 780           | **âˆ’86%**  |

### Google Providers Breakdown

| Provider     | Legacy  | Canonical | Note                            |
|--------------|--------:|----------:|---------------------------------|
| Gemini       | 2,547   | 780       | Unified into 2 files:           |
| Gemini CLI   | 1,520   | (shared)  | `to_ir/gemini.go` (220 lines)   |
| Antigravity  | 1,584   | (shared)  | `from_ir/gemini.go` (560 lines) |
| **Total**    | **5,651** | **780** | **âˆ’86%**                        |

## Provider Support

| Provider      | Parsing (to_ir)      | Generation (from_ir) |
|---------------|:--------------------:|:--------------------:|
| OpenAI        | âœ… Req/Resp/Stream   | âœ… Req/Resp/Stream   |
| Claude        | âœ… Req/Resp/Stream   | âœ… Req/Resp/SSE      |
| Gemini        | âœ… Resp/Stream       | âœ… Req/Resp/Stream   |
| Gemini CLI    | âœ… (shared w/ Gemini)| âœ… GeminiCLIProvider |
| Antigravity   | âœ… (shared w/ Gemini)| âœ… (via GeminiCLI)   |
| Ollama        | âœ… Req/Resp/Stream   | âœ… Req/Resp/Stream   |
| Kiro          | âœ… Resp/Stream       | âœ… Req               |
| Cline         | âœ… (via OpenAI)      | âœ… (via OpenAI)      |

**Cline** â€” provider with free models (MiniMax M2, Grok Code Fast 1), uses OpenAI-compatible format.

**Kiro (Amazon Q)** â€” new provider with access to Claude models via Amazon Q:
- Claude Sonnet 4.5, Claude 4 Opus, Claude 3.7 Sonnet, Claude 3.5 Sonnet/Haiku
- Uses binary AWS Event Stream protocol

### Ollama as Output Format

Ollama is supported **natively via IR** â€” without intermediate conversion to OpenAI. The proxy acts as an Ollama-compatible server with full API implementation. **Server is recommended to run on standard port 11434** to avoid client compatibility issues.

**Flow:** Ollama client â†’ IR â†’ OpenAI â†’ Provider â†’ IR â†’ Ollama response

**Use case:** IDEs with Ollama support but without OpenAI-compatible API (e.g., Copilot Chat).

## Structure

```
translator_new/
â”œâ”€â”€ ir/           # Core (5 files, 1,239 lines)
â”‚   â”œâ”€â”€ types.go            # UnifiedChatRequest, UnifiedEvent, Message
â”‚   â”œâ”€â”€ util.go             # ID generation, finish reason mapping
â”‚   â”œâ”€â”€ message_builder.go  # Message parsing
â”‚   â”œâ”€â”€ response_builder.go # Response building
â”‚   â””â”€â”€ claude_builder.go   # Claude SSE utilities
â”‚
â”œâ”€â”€ to_ir/        # Parsers (5 files, 1,530 lines)
â”‚   â”œâ”€â”€ openai.go   # Chat Completions + Responses API (+ Cline)
â”‚   â”œâ”€â”€ claude.go   # Messages API
â”‚   â”œâ”€â”€ gemini.go   # AI Studio + CLI + Antigravity
â”‚   â”œâ”€â”€ ollama.go   # /api/chat + /api/generate
â”‚   â””â”€â”€ kiro.go     # Amazon Q
â”‚
â””â”€â”€ from_ir/      # Emitters (5 files, 2,533 lines)
    â”œâ”€â”€ openai.go   # Chat Completions + Responses API (+ Cline)
    â”œâ”€â”€ claude.go   # Messages API + SSE streaming
    â”œâ”€â”€ gemini.go   # GeminiProvider + GeminiCLIProvider
    â”œâ”€â”€ ollama.go   # /api/chat + /api/generate
    â””â”€â”€ kiro.go     # KiroProvider
```

## Key Features

- **Reasoning/Thinking** â€” unified handling of thinking blocks with `reasoning_tokens` tracking
- **Tool Calls** â€” unified ID generation and argument parsing
- **Multimodal** â€” images, PDF, inline data
- **Streaming** â€” SSE (OpenAI/Claude) and NDJSON (Gemini/Ollama)
- **Responses API** â€” full support for `/v1/responses`
- **ThinkingSupport** â€” model metadata for reasoning-capable models


## Limitations and Status

### Testing
- âœ… **Tested:** Cursor, Copilot Chat and similar UI clients
- âš ï¸ **Not tested:** CLI agents (Codex CLI, Aider, etc.)
- âš ï¸ **Claude (Anthropic):** implemented without API access, requires testing

### Antigravity Provider â€” UI Client Testing
| Model | Status | Note |
|-------|:------:|------|
| Claude Sonnet 4.5 | âœ… | Fully tested in Cursor/Copilot Chat |
| Gemini models | âœ… | Fully tested in Cursor/Copilot Chat |
| GPT-OSS | âš ï¸ | **Thinking disabled** â€” model gets stuck in planning loops |

> **TODO:** Fix GPT-OSS thinking mode. The model enters infinite planning loops when thinking is enabled, repeatedly generating the same plan without executing actions. Temporarily disabled via `delete(genConfig, "thinkingConfig")` in `antigravity_executor.go`.

### Executors with Canonical IR Support
| Executor           | Status | Note |
|--------------------|:------:|------|
| gemini             | âœ…     | AI Studio, tested |
| gemini_vertex      | âœ…     | Vertex AI, tested |
| gemini_cli         | âœ…     | Google, tested |
| antigravity        | âœ…     | Google, tested (Claude Sonnet, Gemini) |
| aistudio           | âœ…     | AI Studio, tested |
| openai_compat      | âœ…     | OpenAI-compatible, tested |
| cline              | âœ…     | Free models, tested |
| kiro               | âœ…     | Amazon Q (new translator only) |
| claude             | âš ï¸     | Anthropic â€” not tested |
| **codex**          | âŒ     | Requires migration |
| **qwen**           | âŒ     | Requires migration |
| **iflow**          | âŒ     | Requires migration |

## Authentication for New Providers

> **Note:** Unlike Gemini/Claude (full OAuth2 flow with auto browser opening), Cline and Kiro use a **semi-manual method** â€” tokens are extracted from IDE manually.

### Cline
- Uses long-lived refresh token for authentication
- Refresh token is automatically exchanged for short-lived JWT access token (~10 minutes)
- JWT token is used with `workos:` prefix for API requests
- **Important:** Obtaining the refresh token requires modification of the Cline extension source code

### Kiro (Amazon Q)
- Tokens are automatically loaded from JSON file in auth directory (watcher) if you're logged into Kiro IDE, or can be configured manually
- Supports two authentication methods:
  - **Social auth** (Google, etc.) â€” via `prod.*.auth.desktop.kiro.dev`
  - **IAM/SSO auth** â€” via AWS OIDC endpoint
- Tokens are automatically refreshed via the corresponding endpoint

## Compatibility and Migration

**All changes in this branch do not affect the main system operation** â€” new functionality is activated via feature flags:

| Flag | Description | Default |
|------|-------------|---------|
| `use-canonical-translator` | Enables new IR translation architecture | `false` |
| `show-provider-prefixes` | Visual provider prefixes in model list | `false` |

With `use-canonical-translator: false` the system runs on legacy translator without changes.  
New providers (Kiro, Cline, Ollama API) only work with the flag enabled.

**About provider prefixes:** The `show-provider-prefixes` flag adds visual prefixes (e.g., `[Gemini CLI] gemini-2.5-flash`) to distinguish identical models from different providers. Prefixes are purely cosmetic â€” models can be called with or without the prefix.

**Provider selection:** When calling a model without a prefix (or with prefixes disabled), the system uses **round-robin** â€” providers are selected in turn among available ones. This provides load balancing between multiple accounts/providers with the same model.
